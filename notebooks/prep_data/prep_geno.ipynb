{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bionc21\n",
      "Leipzig Cluster detected!\n",
      "/mnt/archgen/users/hringbauer\n",
      "CPU Count: 40\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import socket as socket\n",
    "import os as os\n",
    "import sys as sys\n",
    "import h5py\n",
    "import matplotlib.cm as cm\n",
    "import itertools as it\n",
    "import multiprocessing as mp\n",
    "import allel\n",
    "\n",
    "socket_name = socket.gethostname()\n",
    "print(socket_name)\n",
    "\n",
    "if socket_name.startswith(\"compute-\"):\n",
    "    print(\"HSM O2 Computational partition detected.\")\n",
    "    path = \"/n/groups/reich/hringbauer/\"  # The Path on Harvard Cluster\n",
    "    sys.path.append(\"/n/groups/reich/hringbauer/git/hapBLOCK/python3/\")\n",
    "\n",
    "elif socket_name.startswith(\"bionc\"):\n",
    "    print(\"Leipzig Cluster detected!\")\n",
    "    path = \"/mnt/archgen/users/hringbauer/\"\n",
    "    sys.path.append(\"/mnt/archgen/users/hringbauer/git/hapBLOCK/python3/\")\n",
    "    sys.path.insert(0, \"/mnt/archgen/users/hringbauer/git/HAPSBURG/package/\")\n",
    "else: \n",
    "    raise RuntimeWarning(\"Not compatible machine. Check!!\")\n",
    "\n",
    "os.chdir(path)  # Set the right Path (in line with Atom default)\n",
    "print(os.getcwd())\n",
    "print(f\"CPU Count: {mp.cpu_count()}\")\n",
    "\n",
    "from hapsburg.PackagesSupport.h5_python.h5_functions import merge_in_ld_map\n",
    "from IO.h5_modify import merge_in_af, get_af, get_af1000G, lift_af, save_h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions Harvard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_1240kmarkers(snp1240k_path=\"\", marker_path=\"\", ch=0):\n",
    "    \"\"\"Save all 1240 Markers of .snp eigenstrat file.\n",
    "    to marker_path.\n",
    "    ch: Chromosome. If null filter all of them\"\"\"\n",
    "    df_snp = pd.read_csv(snp1240k_path, header=None, sep=r\"\\s+\", engine=\"python\")\n",
    "    df_snp.columns = [\"SNP\", \"chr\", \"map\", \"pos\", \"ref\", \"alt\"]\n",
    "    if ch>0:\n",
    "        df_snp = df_snp[df_snp[\"chr\"] == ch]\n",
    "    print(f\"Loaded {len(df_snp)} Chr.{ch} SNPs.\")\n",
    "\n",
    "    df_save = df_snp[[\"chr\", \"pos\"]]\n",
    "    df_save.to_csv(marker_path, sep=\"\\t\", header=None, index=False)\n",
    "    print(f\"Saved {len(df_save)} 1240k Markers on Chr. {ch} to {marker_path}\")\n",
    "    \n",
    "def save_1240_1000g_kmarkers(ch=3, snp_path=\"\", marker_path=\"\"):\n",
    "    \"\"\"Save all 1240 and 1000G Markers of .snp eigenstrat file.\n",
    "    to marker_path. Loads Ali Path file\n",
    "    snp_path: Where to find the SNPs plus their types\"\"\"\n",
    "    df1=pd.read_csv(snp_path, sep=\"\\t\", header=None)\n",
    "    df1.columns=[\"chr\", \"pos\", \"pos1\", \"ref\", \"alt\", \"type\"]\n",
    "    print(f\"Loaded {len(df1)} SNPs on Chr. {ch}\")\n",
    "    df2 = df1[df1[\"type\"]==\"1kg+1240k\"]\n",
    "    print(f\"Loaded {len(df2)} Chr.{ch} SNPs in both 1240K and 1000G.\")\n",
    "    df_save = df2[[\"chr\", \"pos1\"]]\n",
    "    df_save.to_csv(marker_path, sep=\"\\t\", header=None, index=False)\n",
    "    print(f\"Saved {len(df_save)} 1240k+1000G Markers on Chr. {ch} to {marker_path}\")\n",
    "    \n",
    "def bctools_filter_vcf(in_vcf_path=\"\", out_vcf_path=\"\", marker_path=\"\"):\n",
    "    \"\"\"Same as PLINK, but with bcftools and directly via Marker Positions.\n",
    "    filter_iids: Whether to use the .csv with Indivdiduals to extract\"\"\"\n",
    "    !bcftools view -Oz -o $out_vcf_path -T $marker_path -M2 -v snps $in_vcf_path\n",
    "    print(\"Finished BCF tools filtering.\")\n",
    "    \n",
    "def bctools_filter_vcf_allvariants(in_vcf_path=\"\", out_vcf_path=\"\", marker_path=\"\"):\n",
    "    \"\"\"Same as PLINK, but with bcftools and directly via Marker Positions.\n",
    "    filter_iids: Whether to use the .csv with Indivdiduals to extract\"\"\"\n",
    "    !bcftools view -Oz -o $out_vcf_path -T $marker_path -v snps $in_vcf_path\n",
    "    print(\"Finished BCF tools filtering.\")\n",
    "    \n",
    "def merge_vcfs(in_vcf_paths=[], out_vcf_path=\"\"):\n",
    "    \"\"\"Merges Set of VCFs into one VCF. \n",
    "    in_vcf_paths: List of VCFs to merge\n",
    "    out_vcf_path: Output of VCF\"\"\"\n",
    "    paths_merge = \" \".join(in_vcf_paths)\n",
    "    !bcftools concat -n -o $out_vcf_path $paths_merge\n",
    "    print(\"Finished BCF tools filtering.\")\n",
    "    \n",
    "##############################################################\n",
    "### Function Identical to vcf_to_hdf5.py in cluster/ folder\n",
    "\n",
    "def vcf_to_1240K_hdf(in_vcf_path = \"/n/groups/reich/ali/WholeGenomeImputation/imputed/v43.4/chr3.bcf\",\n",
    "                     path_vcf = \"./data/vcf/1240k_v43/ch3.vcf.gz\",\n",
    "                     path_h5 = \"./data/hdf5/1240k_v43/ch3.h5\",\n",
    "                     marker_path=\"./data/filters/ho_snps_bcftools_ch3.csv\",\n",
    "                     map_path=\"/n/groups/reich/DAVID/V43/V43.5/v43.5.snp\",\n",
    "                     ch=3):\n",
    "    \"\"\"Convert Ali's vcf to 1240K hdf5. \n",
    "    If marker_path empty, no SNP filtering done.\n",
    "    If map_path empty, no genetic map is merged in.\n",
    "    \"\"\" \n",
    "    if len(marker_path)>0:\n",
    "        bctools_filter_vcf(in_vcf_path = in_vcf_path,\n",
    "                           out_vcf_path= path_vcf,\n",
    "                           marker_path = marker_path)\n",
    "    else: path_vcf = in_vcf_path # Use the unfiltered input in next step\n",
    "    print(\"Finished downsampling to 1240K\")\n",
    "    \n",
    "    allel.vcf_to_hdf5(input=path_vcf, output=path_h5, \n",
    "                  fields = ['variants/*', 'calldata/*', \"samples\"], compression=\"gzip\") # Do the conversion to hdf5. Takes hours\n",
    "    print(\"Finished conversion to hdf5!\")\n",
    "    \n",
    "    if len(map_path)>0:\n",
    "        merge_in_ld_map(path_h5=path_h5, \n",
    "                    path_snp1240k=map_path,\n",
    "                    chs=[ch])\n",
    "        \n",
    "        \n",
    "########################################################\n",
    "### New Functions for Leipzig Processing\n",
    "\n",
    "def get_paths_ch(path_bcfs=\"\", ch=3):\n",
    "    \"\"\"Return all unique iid paths of bcfs with chromosome ch.\"\"\"\n",
    "    files = os.listdir(path_bcfs)\n",
    "    iids = [f.split(\".\")[0] for f in files]\n",
    "    k = len(set(iids)) \n",
    "    print(f\"{k} unique iids \")\n",
    "    \n",
    "    path_chs = [f for f in files if (f\"chr{ch}.\" in f)]\n",
    "    k1 = len(path_chs) \n",
    "    print(f\"{k1} iids for Chr. {ch} found\")\n",
    "    assert(len(path_chs)==k)\n",
    "    paths_full = [os.path.join(path_bcfs, p) for p in path_chs]\n",
    "    return paths_full\n",
    "\n",
    "def merge_grg_bcfs(path_bcfs = \"/mnt/archgen/users/childebayeva/GRG/GLIMPSE_MERGED_1240K\",\n",
    "               out_path = \"/mnt/archgen/users/hringbauer/data/GRG/bcf_allv1/\", ch = 3,\n",
    "               index = True):\n",
    "    \"\"\"Merge imputed bcf files from Gurgy. \n",
    "    Output: one combined bcf file in out_path.\n",
    "    Takes about 5 min for long Chromosome\"\"\"\n",
    "    out_path = f\"{out_path}ch{ch}.bcf\"\n",
    "    paths_full = get_paths_ch(path_bcfs, ch=ch)\n",
    "\n",
    "    vcfs = \" \".join(paths_full)  # Get all vcfs in space seperated list\n",
    "\n",
    "    ### Index Files: Takes about 1min for 100 samples\n",
    "    if index:\n",
    "        for p in paths_full[:]:\n",
    "            !bcftools index $p\n",
    "    print(\"Merging bcfs...\")\n",
    "    !bcftools merge -o $out_path $vcfs # Takes ~30 s for long Chromosome\n",
    "    print(f\"Fnished Merging. Output @ {out_path}\")\n",
    "    \n",
    "def process_h5_ainash(ch = 3, h5_path=\"\", h5_path_save=\"\"):\n",
    "    \"\"\"Merges H5 that Ainash created, by bringing together\n",
    "    GT and GP fields into one framework\"\"\"\n",
    "    with h5py.File(h5_path, \"r\") as f:  # Load for Sanity Check. See below!\n",
    "        samples = f[\"samples\"][:]\n",
    "        samples = [\".\".join(s.split(\".\")[:2]) for s in samples[::2]]\n",
    "\n",
    "        save_h5(gt=f[\"calldata/GT\"][:,1::2,:],\n",
    "                gp = f[\"calldata/GP\"][:,::2,:],\n",
    "                ad =  [], ad_group=False,\n",
    "                ref=f[\"variants/REF\"][:], alt=f[\"variants/ALT\"][:,0],\n",
    "                pos=f[\"variants/POS\"][:], rec=f[\"variants/MAP\"][:],\n",
    "                samples=samples, path=h5_path_save,\n",
    "                compression='gzip', gt_type='int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform individual .bcfs to summary HDF5\n",
    "Takes about 150 Seconds for long chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 unique iids \n",
      "94 iids for Chr. 1 found\n",
      "Merging bcfs...\n",
      "Fnished Merging. Output @ /mnt/archgen/users/hringbauer/data/GRG/bcf_allv1/ch1.bcf\n",
      "Finished downsampling to 1240K\n",
      "Finished conversion to hdf5!\n",
      "Lifting LD Map from eigenstrat to HDF5...\n",
      "Loaded 89147 variants.\n",
      "Loaded 188 individuals.\n",
      "Loaded 93166 Chr.1 1240K SNPs.\n",
      "Intersection 89147 out of 89147 HDF5 SNPs\n",
      "Finished Chromosome 1.\n",
      "Adding map to HDF5...\n",
      "We did it. Finished.\n",
      "Successfully saved 94 individuals to: /mnt/archgen/users/hringbauer/data/GRG/hdf5/processed/ch1.h5\n",
      "Finished running chromosome 1. GZ\n",
      "94 unique iids \n",
      "94 iids for Chr. 2 found\n",
      "Merging bcfs...\n",
      "Fnished Merging. Output @ /mnt/archgen/users/hringbauer/data/GRG/bcf_allv1/ch2.bcf\n",
      "Finished downsampling to 1240K\n",
      "Finished conversion to hdf5!\n",
      "Lifting LD Map from eigenstrat to HDF5...\n",
      "Loaded 94239 variants.\n",
      "Loaded 188 individuals.\n",
      "Loaded 98657 Chr.2 1240K SNPs.\n",
      "Intersection 94239 out of 94239 HDF5 SNPs\n",
      "Finished Chromosome 2.\n",
      "Adding map to HDF5...\n",
      "We did it. Finished.\n",
      "Successfully saved 94 individuals to: /mnt/archgen/users/hringbauer/data/GRG/hdf5/processed/ch2.h5\n",
      "Finished running chromosome 2. GZ\n",
      "CPU times: user 1min 15s, sys: 4.74 s, total: 1min 20s\n",
      "Wall time: 4min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for ch in range(1,23):\n",
    "    ### Merge the Gurgy Data into one bcf\n",
    "    merge_grg_bcfs(path_bcfs = \"/mnt/archgen/users/childebayeva/GRG/GLIMPSE_MERGED_1240K\",\n",
    "               out_path = \"/mnt/archgen/users/hringbauer/data/GRG/bcf_allv1/\", ch = ch,\n",
    "               index = True)\n",
    "    \n",
    "    in_vcf_path = f\"/mnt/archgen/users/hringbauer/data/GRG/bcf_allv1/ch{ch}.bcf\"\n",
    "    path_h5 = f\"/mnt/archgen/users/hringbauer/data/GRG/hdf5/raw/ch{ch}.h5\"\n",
    "    path_h5_save = f\"/mnt/archgen/users/hringbauer/data/GRG/hdf5/processed/ch{ch}.h5\"\n",
    "    map_path = f\"/mnt/archgen/users/hringbauer/data/MinMyc.snp\"\n",
    "    \n",
    "    vcf_to_1240K_hdf(in_vcf_path = in_vcf_path, path_vcf = \"\", path_h5=path_h5,\n",
    "                     marker_path = \"\", map_path = map_path, ch=ch)\n",
    "    \n",
    "    ### Create processed HDF5\n",
    "    process_h5_ainash(h5_path=path_h5, h5_path_save=path_h5_save)\n",
    "\n",
    "    print(f\"Finished running chromosome {ch}. GZ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some random test lines\n",
    "p1 = os.path.join(path_bcfs, out_path)\n",
    "!bcftools query -l $p1 | wc -l\n",
    "##!bcftools query -f '%POS\\n' $p1 | wc -l\n",
    "#!bcftools view $p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test HDF5 that was created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calldata', 'samples', 'variants']\n",
      "['GP', 'GT']\n",
      "['ALT', 'MAP', 'POS', 'REF']\n",
      "(77652, 94, 2)\n",
      "[[0 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File(\"/mnt/archgen/users/hringbauer/data/GRG/hdf5/processed/ch3.h5\", \"r\")  # Load for Sanity Check. See below!\n",
    "\n",
    "print(list(f)) # Gives highest level folder\n",
    "print(list(f[\"calldata\"]))\n",
    "print(list(f[\"variants\"]))\n",
    "print(np.shape(f[\"calldata/GT\"]))\n",
    "gt1 = f[\"calldata/GT\"][:10,0,:] # Gets first ten loci for individual 0\n",
    "#print(np.shape(f[\"calddata/GT\"]))\n",
    "f.close()\n",
    "\n",
    "print(gt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77652, 94, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GRG089.A01', 'GRG053.A01', 'GRG088.A01', 'GRG043.A01',\n",
       "       'GRG090.A01', 'GRG074.A01', 'GRG028.A01', 'GRG097.A01',\n",
       "       'GRG064.A01', 'GRG108.A01', 'GRG087.A01', 'GRG009.B01',\n",
       "       'GRG067.A01', 'GRG032.A01', 'GRG034.A01', 'GRG037.A01',\n",
       "       'GRG019.A01', 'GRG080.A01', 'GRG063.A01', 'GRG048.A01',\n",
       "       'GRG071.A01', 'GRG109.A01', 'GRG014.A01', 'GRG107.A01',\n",
       "       'GRG047.A01', 'GRG077.A01', 'GRG073.A01', 'GRG035.A01',\n",
       "       'GRG095.A01', 'GRG033.A01', 'GRG016.A01', 'GRG054.B01',\n",
       "       'GRG086.A01', 'GRG011.A01', 'GRG022.A01', 'GRG025.A01',\n",
       "       'GRG052.A01', 'GRG023.A01', 'GRG091.A01', 'GRG004.A01',\n",
       "       'GRG059.A01', 'GRG029.A01', 'GRG024.A01', 'GRG076.A01',\n",
       "       'GRG075.A01', 'GRG017.A01', 'GRG066.A01', 'GRG046.A01',\n",
       "       'GRG051.B01', 'GRG103.A01', 'GRG006.B01', 'GRG030.A01',\n",
       "       'GRG018.A01', 'GRG039.A01', 'GRG005.B01', 'GRG056.A01',\n",
       "       'GRG060.B01', 'GRG045.B01', 'GRG065.A01', 'GRG096.A01',\n",
       "       'GRG008.A01', 'GRG068.A01', 'GRG010.A01', 'GRG050.A01',\n",
       "       'GRG079.A01', 'GRG081.A01', 'GRG027.A01', 'GRG038.A01',\n",
       "       'GRG078.A01', 'GRG036.A01', 'GRG001.A01', 'GRG031.A01',\n",
       "       'GRG070.A01', 'GRG041.A01', 'GRG012.A01', 'GRG085.A01',\n",
       "       'GRG094.A01', 'GRG013.A01', 'GRG084.A01', 'GRG061.A01',\n",
       "       'GRG049.A01', 'GRG015.A02', 'GRG021.A01', 'GRG069.A01',\n",
       "       'GRG026.A01', 'GRG042.A01', 'GRG002.A01', 'GRG102.all',\n",
       "       'GRG057.A01', 'GRG020.A01', 'GRG007.A01', 'GRG058.A01',\n",
       "       'GRG003.B01', 'GRG055.A01'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[\"samples\"][:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
